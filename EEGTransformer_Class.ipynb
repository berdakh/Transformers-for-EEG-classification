{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Supplementary Material\n",
        "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "\n",
        "\n",
        "### EEGTransformer Class\n",
        "\n",
        "The `EEGTransformer` class is designed to leverage a transformer-based architecture tailored specifically for Electroencephalogram (EEG) data processing.\n",
        "\n",
        "#### Parameters:\n",
        "- `num_channels` (int): Specifies the number of channels in the EEG dataset.\n",
        "- `num_timepoints` (int): Indicates the number of time points or the sequence length in the EEG data.\n",
        "- `output_dim` (int): Defines the output dimensionality for the classifier layer.\n",
        "- `hidden_dim` (int): Specifies the hidden layer dimensionality.\n",
        "- `num_heads` (int): Determines the number of attention heads to be used in the multi-head self-attention mechanism.\n",
        "- `key_query_dim` (int): Denotes the dimensionality for the key/query pairs in the self-attention mechanism.\n",
        "- `hidden_ffn_dim` (int): Indicates the hidden layer dimensionality for the feed-forward network.\n",
        "- `intermediate_dim` (int): Refers to the dimensionality of the intermediate layer in the feed-forward network.\n",
        "- `ffn_output_dim` (int): Specifies the output size of the feed-forward network.\n",
        "\n",
        "#### Attributes:\n",
        "- `positional_encoding` (torch.Tensor): A tensor of shape `(num_channels, num_timepoints)` that imparts the sequence position information.\n",
        "- `multihead_attn` (nn.MultiheadAttention): Implements the multi-head self-attention mechanism.\n",
        "- `ffn` (nn.Sequential): Constructs a feed-forward network composed of a linear transformation followed by ReLU activation and another linear transformation.\n",
        "- `norm1` and `norm2` (nn.LayerNorm): Execute layer normalization.\n",
        "- `classifier` (nn.Linear): Deploys a final linear transformation layer to categorize the input into designated classes.\n",
        "\n",
        "#### Methods:\n",
        "- `forward(X)`: Outlines the forward propagation for the model.\n",
        "  - `X` (torch.Tensor): The input tensor for EEG data, which should have a shape of `(batch_size, num_channels, num_timepoints)`.\n",
        "\n",
        "  - **Steps**:\n",
        "    1. Standardize the input tensor.\n",
        "    2. Apply positional encoding.\n",
        "    3. Implement multi-head self-attention.\n",
        "    4. Reshape the attention output and apply layer normalization.\n",
        "    5. Forward the data through the feed-forward network.\n",
        "    6. Flatten the resultant tensor and direct it through a classifier layer.\n",
        "    7. Yield the final output.\n",
        "  \n",
        "### Notes:\n",
        "\n",
        "- The model applies layer normalization after the multi-head self-attention and feed-forward network stages.\n",
        "- Positional encoding is utilized to impart sequence position information to the model, which can either be relative or absolute.\n",
        "- The classifier layer flattens the model output and categorizes it into `output_dim` classes.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "To employ the `EEGTransformer` model, instantiate the class using the desired parameters. Then, similar to any other PyTorch model, forward the input data to the model and utilize the returned output for either training or inference.\n",
        "\n",
        "```python\n",
        "# Sample Usage\n",
        "model = EEGTransformer(num_channels=32, num_timepoints=200, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=32)\n",
        "                       \n",
        "input_data = torch.randn(64, 32, 200)\n",
        "output = model(input_data)\n",
        "```\n",
        "\n",
        "Ensure that the model is paired with a compatible loss function and optimizer for effective training. Depending on the specifics of the EEG dataset or application requirements, the model can be further refined."
      ],
      "metadata": {
        "id": "IMTA3qNexKv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZgbQrxxFbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class EEGTransformer(nn.Module):\n",
        "    def __init__(self, num_channels, num_timepoints, output_dim,\n",
        "                 hidden_dim, num_heads, key_query_dim,\n",
        "                 hidden_ffn_dim, intermediate_dim, ffn_output_dim):\n",
        "        super(EEGTransformer, self).__init__()\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = torch.zeros(num_channels, num_timepoints)\n",
        "        for j in range(num_channels):\n",
        "            for k in range(num_timepoints):\n",
        "                if j % 2 == 0:\n",
        "                    self.positional_encoding[j][k] =\\\n",
        "                        torch.sin(torch.tensor(k) / (10000 ** (torch.tensor(j) / num_channels)))\n",
        "                else:\n",
        "                    self.positional_encoding[j][k] =\\\n",
        "                        torch.cos(torch.tensor(k) / (10000 ** ((torch.tensor(j) - 1) / num_channels)))\n",
        "\n",
        "        # Multi-Head Self Attention\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=num_channels,\n",
        "                                                    num_heads=num_heads)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(num_channels, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(intermediate_dim, ffn_output_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(num_channels)\n",
        "        self.norm2 = nn.LayerNorm(num_channels)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(num_channels * num_timepoints, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Input Standardization\n",
        "        mean = X.mean(dim=2, keepdim=True)\n",
        "        std  = X.std(dim=2, keepdim=True)\n",
        "        X_hat = (X - mean) / (std + 1e-5)  # epsilon to avoid division by zero\n",
        "\n",
        "        # Add Positional Encoding\n",
        "        X_tilde = X_hat + self.positional_encoding.to(X.device)\n",
        "\n",
        "        # Reshape for multi-head self attention: (seq_len, batch_size, embed_dim)\n",
        "        X_tilde = X_tilde.permute(2, 0, 1)\n",
        "\n",
        "        # Multi-Head Self Attention\n",
        "        attn_output, _ = self.multihead_attn(X_tilde, X_tilde, X_tilde)\n",
        "\n",
        "        # Reshape back and Apply Layer Norm\n",
        "        # attn_output = attn_output.permute(1, 2, 0)  # Reshape: (batch_size, embed_dim, seq_len)\n",
        "        X_ring = torch.stack([self.norm1(a) for a in attn_output], dim=1)\n",
        "\n",
        "        # Position-wise Feed-Forward Networks\n",
        "        ff_output = self.ffn(X_ring)\n",
        "        O = self.norm2(ff_output + X_ring)\n",
        "\n",
        "        # Classifier\n",
        "        # Flatten and classify\n",
        "        O_flat = O.view(O.size(0), -1)  # Flatten the tensor\n",
        "        output = self.classifier(O_flat)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample generated data (replace with real EEG (segmented) data)\n",
        "num_channels=32\n",
        "num_timepoints=200\n",
        "batch_size = 64\n",
        "\n",
        "X = torch.randn(batch_size, num_channels, num_timepoints)\n",
        "y = torch.randint(0, 2, (batch_size,))  # L=2 for binary classification\n",
        "\n",
        "# Model, Loss and Optimizer\n",
        "model = EEGTransformer(num_channels, num_timepoints, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=num_channels)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# once the model is trained, it can be tested on unseen EEG test examples\n",
        "# also, different model selection techniques (e.g. cross-validation methods) can be implemented within the training loop\n"
      ],
      "metadata": {
        "id": "8FJBCrm0xUWA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}