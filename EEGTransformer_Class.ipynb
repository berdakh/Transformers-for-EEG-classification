{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTA3qNexKv3"
      },
      "source": [
        "## Supplementary Material\n",
        "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "\n",
        "\n",
        "### EEGTransformer Class\n",
        "\n",
        "The `EEGTransformer` class is designed to leverage a transformer-based architecture tailored specifically for Electroencephalogram (EEG) data processing.\n",
        "\n",
        "#### Parameters:\n",
        "- `num_channels` (int): Specifies the number of channels in the EEG dataset.\n",
        "- `num_timepoints` (int): Indicates the number of time points or the sequence length in the EEG data.\n",
        "- `output_dim` (int): Defines the output dimensionality for the classifier layer.\n",
        "- `hidden_dim` (int): Specifies the hidden layer dimensionality.\n",
        "- `num_heads` (int): Determines the number of attention heads to be used in the multi-head self-attention mechanism.\n",
        "- `key_query_dim` (int): Denotes the dimensionality for the key/query pairs in the self-attention mechanism.\n",
        "- `hidden_ffn_dim` (int): Indicates the hidden layer dimensionality for the feed-forward network.\n",
        "- `intermediate_dim` (int): Refers to the dimensionality of the intermediate layer in the feed-forward network.\n",
        "- `ffn_output_dim` (int): Specifies the output size of the feed-forward network.\n",
        "\n",
        "#### Attributes:\n",
        "- `positional_encoding` (torch.Tensor): A tensor of shape `(num_channels, num_timepoints)` that imparts the sequence position information.\n",
        "- `multihead_attn` (nn.MultiheadAttention): Implements the multi-head self-attention mechanism.\n",
        "- `ffn` (nn.Sequential): Constructs a feed-forward network composed of a linear transformation followed by ReLU activation and another linear transformation.\n",
        "- `norm1` and `norm2` (nn.LayerNorm): Execute layer normalization.\n",
        "- `classifier` (nn.Linear): Deploys a final linear transformation layer to categorize the input into designated classes.\n",
        "\n",
        "#### Methods:\n",
        "- `forward(X)`: Outlines the forward propagation for the model.\n",
        "  - `X` (torch.Tensor): The input tensor for EEG data, which should have a shape of `(batch_size, num_channels, num_timepoints)`.\n",
        "\n",
        "  - **Steps**:\n",
        "    1. Standardize the input tensor.\n",
        "    2. Apply positional encoding.\n",
        "    3. Implement multi-head self-attention.\n",
        "    4. Reshape the attention output and apply layer normalization.\n",
        "    5. Forward the data through the feed-forward network.\n",
        "    6. Flatten the resultant tensor and direct it through a classifier layer.\n",
        "    7. Yield the final output.\n",
        "  \n",
        "### Notes:\n",
        "\n",
        "- The model applies layer normalization after the multi-head self-attention and feed-forward network stages.\n",
        "- Positional encoding is utilized to impart sequence position information to the model, which can either be relative or absolute.\n",
        "- The classifier layer flattens the model output and categorizes it into `output_dim` classes.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "To employ the `EEGTransformer` model, instantiate the class using the desired parameters. Then, similar to any other PyTorch model, forward the input data to the model and utilize the returned output for either training or inference.\n",
        "\n",
        "```python\n",
        "# Sample Usage\n",
        "model = EEGTransformer(num_channels=32, num_timepoints=200, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=32)\n",
        "                       \n",
        "input_data = torch.randn(64, 32, 200)\n",
        "output = model(input_data)\n",
        "```\n",
        "\n",
        "Ensure that the model is paired with a compatible loss function and optimizer for effective training. Depending on the specifics of the EEG dataset or application requirements, the model can be further refined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZgbQrxxFbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class EEGTransformer(nn.Module):\n",
        "    def __init__(self, num_channels, num_timepoints, output_dim,\n",
        "                 hidden_dim, num_heads, key_query_dim,\n",
        "                 hidden_ffn_dim, intermediate_dim, ffn_output_dim):\n",
        "        super(EEGTransformer, self).__init__()\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = torch.zeros(num_channels, num_timepoints)\n",
        "        for j in range(num_channels):\n",
        "            for k in range(num_timepoints):\n",
        "                if j % 2 == 0:\n",
        "                    self.positional_encoding[j][k] =\\\n",
        "                        torch.sin(torch.tensor(k) / (10000 ** (torch.tensor(j) / num_channels)))\n",
        "                else:\n",
        "                    self.positional_encoding[j][k] =\\\n",
        "                        torch.cos(torch.tensor(k) / (10000 ** ((torch.tensor(j) - 1) / num_channels)))\n",
        "\n",
        "        # Multi-Head Self Attention\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=num_channels,\n",
        "                                                    num_heads=num_heads)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(num_channels, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(intermediate_dim, ffn_output_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(num_channels)\n",
        "        self.norm2 = nn.LayerNorm(num_channels)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(num_channels * num_timepoints, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Input Standardization\n",
        "        mean = X.mean(dim=2, keepdim=True)\n",
        "        std  = X.std(dim=2, keepdim=True)\n",
        "        X_hat = (X - mean) / (std + 1e-5)  # epsilon to avoid division by zero\n",
        "\n",
        "        # Add Positional Encoding\n",
        "        X_tilde = X_hat + self.positional_encoding.to(X.device)\n",
        "\n",
        "        # Reshape for multi-head self attention: (seq_len, batch_size, embed_dim)\n",
        "        X_tilde = X_tilde.permute(2, 0, 1)\n",
        "\n",
        "        # Multi-Head Self Attention\n",
        "        attn_output, _ = self.multihead_attn(X_tilde, X_tilde, X_tilde)\n",
        "\n",
        "        # Reshape back and Apply Layer Norm\n",
        "        # attn_output = attn_output.permute(1, 2, 0)  # Reshape: (batch_size, embed_dim, seq_len)\n",
        "        X_ring = torch.stack([self.norm1(a) for a in attn_output], dim=1)\n",
        "\n",
        "        # Position-wise Feed-Forward Networks\n",
        "        ff_output = self.ffn(X_ring)\n",
        "        O = self.norm2(ff_output + X_ring)\n",
        "\n",
        "        # Classifier\n",
        "        # Flatten and classify\n",
        "        O_flat = O.view(O.size(0), -1)  # Flatten the tensor\n",
        "        output = self.classifier(O_flat)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FJBCrm0xUWA"
      },
      "outputs": [],
      "source": [
        "# Sample generated data (replace with real EEG (segmented) data)\n",
        "num_channels=32\n",
        "num_timepoints=200\n",
        "batch_size = 64\n",
        "\n",
        "X = torch.randn(batch_size, num_channels, num_timepoints)\n",
        "y = torch.randint(0, 2, (batch_size,))  # L=2 for binary classification\n",
        "\n",
        "# Model, Loss and Optimizer\n",
        "model = EEGTransformer(num_channels, num_timepoints, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=num_channels)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# once the model is trained, it can be tested on unseen EEG test examples\n",
        "# also, different model selection techniques (e.g. cross-validation methods) can be implemented within the training loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer's Architecture for EEG Classification\n",
        "\n",
        "This section presents the \"standard\" approach for utilizing the Transformer encoder to classify EEG patterns for BCIs.\n",
        "\n",
        "#### Input Standardization and Positional Encoding\n",
        "\n",
        "Let the set of pairs $ D_{\\text{train}} = \\{(\\mathbf{X}_1,{y}_1),\\dots, (\\mathbf{X}_n,{y}_n)\\} $ denote $ n $ trials of EEG recordings where $ {y}_i $ is the scaler class variable with $ L $ possible labels (e.g., target and non-target in a binary classification) and $ \\mathbf{X}_i\\in \\mathbb{R}^{c\\times p} $ is the collection of EEG observations in the $ i^{\\text{th}} $ trial over $ c $ channels and $ p $ time points; that is to say,\n",
        "\n",
        "$$ \\mathbf{X}_i=[\\mathbf{x}_{i1},\\mathbf{x}_{i2},\\ldots,\\mathbf{x}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        " \n",
        "with $ \\mathbf{x}_{ij}=[x_{ij1}, \\ldots, x_{ijp}]^T \\in \\mathbb{R}^{p\\times 1}, j=1,\\ldots,c $, where $ x_{ijk}, k=1, \\ldots, p $ denotes the $ k^{\\text{th}} $ element of vector $ \\mathbf{x}_{ij} $, and $ T $ denotes the transpose operator. The goal is to use $ D_{\\text{train}} $ and train a classifier $ \\psi: \\mathbb{R}^{c\\times p} \\rightarrow \\{0, 1, \\ldots, L-1\\} $ that maps a given $ \\mathbf{X} $ to a possible value of the class variable.\n",
        "\n",
        "It is common to apply standardization for each channel to make the sensory data across all channels comparable. \n",
        "In this regard, each $ \\mathbf{X}_i $ is converted to $ \\hat{\\mathbf{X}}_i $ where \n",
        "\n",
        "$$ \\hat{\\mathbf{X}}_i=[\\hat{\\mathbf{x}}_{i1},\\hat{\\mathbf{x}}_{i2},\\ldots,\\hat{\\mathbf{x}}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        "\n",
        "and where $ \\hat{\\mathbf{x}}_{ij} = [\\hat{x}_{ij1}, \\ldots, \\hat{x}_{ijp}]^T $ such that \n",
        "\n",
        "$$ \\hat{x}_{ijk} = \\frac{{x}_{ijk}-m_{ij}}{s_{ij}}\\,, $$\n",
        "\n",
        "with $ m_{ij} $ and $ s_{ij} $ being the sample mean and sample standard deviation of vector $\\mathbf{x}_{ij} $ given by\n",
        "\n",
        "$$ m_{ij} = \\frac{1}{p} \\sum_{k=1}^p {x}_{ijk}\\,, $$\n",
        "$$ s_{ij} = \\sqrt{\\frac{1}{p} \\sum_{k=1}^p ({x}_{ijk}-m_{ij})^2}\\,, $$\n",
        "\n",
        "respectively.\n",
        "\n",
        "In order for the Transformer to make use of EEG recording orders, it is common to encode some information about the position of sequence elements in its input \\cite{vaswani_attention_2017}. This positional encoding is generally realized by adding each $ \\hat{\\mathbf{X}}_i $ to a matrix $ \\mathbf{P} \\in \\mathbb{R}^{c\\times p} $ that is defined based on trigonometric functions with different frequencies for each channel \\cite{vaswani_attention_2017}. As a result, we obtain\n",
        "\n",
        "$$ \\tilde{\\mathbf{X}}_i = \\hat{\\mathbf{X}}_i + \\mathbf{P}, \\,i=1,\\ldots, n, $$\n",
        "\n",
        "where the element on row (channel) $ j=1,\\ldots, c $, and column (time index) $ k=1, \\ldots, p $, of $ \\mathbf{P} $, denoted $ p_{jk} $ is given by\n",
        "\n",
        "$$ p_{jk} = \\begin{cases}\n",
        "\\text{\n",
        "\n",
        "sin}\\Big(k/10000^{j/c} \\Big), & \\text{for even } j \\\\\n",
        "\\text{cos}\\Big(k/10000^{j-1/c} \\Big), & \\text{for odd } j\n",
        "\\end{cases} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Self-Attention Mechanisms: Capturing Contexts for EEG Classification\n",
        "\n",
        "Capturing contexts is the essential concept that makes attention mechanism a promising operation for EEG classification. A context is simply another representation of an element of the input sequence (here one column of each $ \\tilde{\\mathbf{X}}_i $) based on its compatibility with other elements within the sequence. The most widely used attention operation for EEG classification is scaled dot-product self-attention, denoted $ \\text{SA}^d_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}(\\tilde{\\mathbf{X}}_i): \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d\\times p} $, which was initially proposed and used for translation tasks \\cite{vaswani_attention_2017}. In particular,\n",
        "\n",
        "$$ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) = \\mathbf{V}\\tilde{\\mathbf{X}}_i\\times\\text{softmax}\\Big(\\frac{\\tilde{\\mathbf{X}}_i^T\\mathbf{K}^T\\mathbf{Q}\\tilde{\\mathbf{X}}_i}{\\sqrt{q}}\\Big)\\,, $$\n",
        "\n",
        "where $ \\mathbf{V} \\in \\mathbb{R}^{d\\times c} $, $ \\mathbf{K} \\in \\mathbb{R}^{q\\times c} $, $ \\mathbf{Q} \\in \\mathbb{R}^{q\\times c} $ are projection matrices that are learned in the training process, $ q $ is known as attention dimensionality, and  $ d $, which is generally a tuning parameter, denotes the dimensionality of the columns of the output matrix (context vectors). We use superscript $ d $ in $ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) $ to highlight the dimensionality of context vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Multi-Head Self-Attention\n",
        "\n",
        "Rather than a single self-attention operation, it is generally beneficial to apply multiple self-attentions in parallel. Using this operation, we view the compatibility of sequence elements using different learned projections. In this context, it is also common to refer to the output matrix of each self-attention as a head. In particular, the multi-head self-attention, denoted $ \\text{MSHA}(\\tilde{\\mathbf{X}}_i) : \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d_h\\times p} $, is defined as\n",
        "\n",
        "$$ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) = \\mathbf{W}[\\text{SA}^d_{\\mathbf{V}_1, \\mathbf{K}_1, \\mathbf{Q}_1}(\\tilde{\\mathbf{X}}_i)^T, \\ldots, \\text{SA}^d_{\\mathbf{V}_m, \\mathbf{K}_m, \\mathbf{Q}_m}(\\tilde{\\mathbf{X}}_i)^T]^T\\,, $$\n",
        "\n",
        "where $ \\mathbf{W}\\in \\mathbb{R}^{d_h\\times md} $ is another learnable projection matrix, $ m $ is the number of self-attentions used in (\\ref{MHSA}), which is also known as the number of heads, and $ d_h $ is the dimensionality of columns in the output of $ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) $ operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Identity Skip-Connection and Layer Normalization\n",
        "\n",
        "To ensure the stability and efficacy of the training process, especially with the complex nature of EEG data, the Transformer encoder utilizes identity skip-connections \\cite{he_deep_2015} followed by layer normalization \\cite{Jimmy_2016}. Here we define these operations. Let $ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big): \\mathbb{R}^{a\\times b}\\rightarrow \\mathbb{R}^{a\\times b}$ denote the identity skip-connection around a layer $\\text{LAY}(\\mathbf{Y}) $ (an operation) that operates on an input $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ to produce an output of the same size as the input. Then\n",
        "\n",
        "$$ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big) = \\mathbf{Y} + \\text{LAY}(\\mathbf{Y})\\,. $$\n",
        "\n",
        "That is to say, we simply add the output of $ \\text{LAY}(\\mathbf{Y}) $ to its input. \n",
        "Furthermore, let $\\text{LN}(\\mathbf{Y}):\\mathbb{R}^{a\\times b} \\rightarrow \\mathbb{R}^{a\\times b}$ denote the layer normalization applied to an $ (a > 1)\\times b $ matrix $ \\mathbf{Y} $ with elements $ y_{jk}, j=1,\\ldots,a, k=1,\\ldots,b $ where each row records measurements for a \"features\" (here, channel). Then, $ \\text{LN}(\\mathbf{Y}) $ produces $ \\mathring{\\mathbf{Y}} $, which is a matrix of the same size $ \\mathring{\\mathbf{Y}} $ with elements $ \\mathring{y}_{jk} $ where\n",
        "\n",
        "$$ \\mathring{y}_{jk} = \\frac{{y}_{jk}-m_{k}}{s_{k}}\\,, $$\n",
        "\n",
        "and where\n",
        "\n",
        "$$ m_{k} = \\frac{1}{a} \\sum_{j=1}^a {y}_{jk}\\,, $$\n",
        "$$ s_{k} = \\sqrt{\\frac{1}{a} \\sum_{j=1}^a ({y}_{jk}-m_{k})^2}\\,. $$\n",
        "\n",
        "In other words, $ \\mathring{\\mathbf{Y}} $ is a type of standardization where the sample mean and sample standard deviation are computed for each column of $ \\mathbf{Y} $ (in the EEG context means for each time point in the sequence) over all features. One place that these operations are used in the transformer encoder is to produce $ \\mathring{\\mathbf{X}}_i $ as follows:\n",
        "\n",
        "$$ \\mathring{\\mathbf{X}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{MSHA}^{c}(\\tilde{\\mathbf{X}}_i)}\\big)\\Big)\\,; $$\n",
        " \n",
        "\n",
        "that is, the skip-connection is used around the multi-head self-attention, which is then followed by layer normalization. Note that the use of skip-connection in (\\ref{outSKPLN}) enforces setting $ d_h $ defined in (\\ref{MHSA}) to $ c $, which is the number of channels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Position-wise Feed-Forward Networks\n",
        "\n",
        "The Transformer encoder utilizes a fully connected feed-forward network that transforms each element of a given sequence individually. Let $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ be the generic matrix defined before. The effect of this position-wise feed-forward network operated on an input $ \\mathbf{Y} $, denoted \n",
        "$\\text{FFN}(\\mathbf{Y})$, is:\n",
        "\n",
        "$$ \\text{FFN}^s(\\mathbf{Y}) = [g(\\mathbf{y}_1), \\ldots, g(\\mathbf{y}_b)]\\,, $$\n",
        " \n",
        "\n",
        "where $ \\mathbf{y}_k, k=1,\\ldots, b $ are columns of $ \\mathbf{Y} $ and\n",
        "\n",
        "$$ g(\\mathbf{y}_k) = \\mathbf{W}_2\\times f(\\mathbf{W}_1\\mathbf{y}_k + \\mathbf{b}_1) + \\mathbf{b}_2\\,, $$\n",
        "\n",
        "where $ f(.) $ denotes an element-wise nonlinear activation function (e.g., ReLU), and $\\mathbf{W}_1\\in \\mathbb{R}^{r\\times a}$, and $\\mathbf{W}_2 \\in \\mathbb{R}^{s\\times r}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{r\\times 1}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{s\\times 1} $ are learnable matrices and vectors—$ r $ is generally a tuning parameter. \n",
        "\n",
        "We use superscript $ s $ in $ \\text{FFN}^s(\\mathbf{Y}) $ to highlight the dimensionality of output vectors in (\\ref{FFN}). In the Transformer encoder, position-wise feed-forward network is used to produce an output $ {\\mathbf{O}}_i $ from $ \\mathring{\\mathbf{X}}_i $ obtained in (\\ref{outSKPLN}), which is then added to its input through the skip-connection, followed by layer normalization. This operation is characterized as follows:\n",
        "\n",
        "$$ {\\mathbf{O}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{FFN}^{c}(\\mathring{\\mathbf{X}}_i)}\\big)\\Big)\\,. $$\n",
        " \n",
        "\n",
        "Note that the use of skip-connection in (\\ref{outSKPLNN}) enforces setting $ s $ defined in (\\ref{FFN}) to $ c $. The classification can be performed by vectorizing $ {\\mathbf{O}}_i $ and using that as the input to a fully connected layer with a softmax activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_latex_delimiters(text):\n",
        "    # Replace \\( and \\) with $\n",
        "    text = text.replace(r\"\\(\", \"$\").replace(r\"\\)\", \"$\")\n",
        "    \n",
        "    # Replace \\[ and \\] with $$\n",
        "    text = text.replace(r\"\\[\", \"$$\").replace(r\"\\]\", \"$$\")\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "# Read input text\n",
        "with open('eq.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Apply the replacement\n",
        "modified_content = replace_latex_delimiters(content)\n",
        "\n",
        " \n",
        "with open('output.txt', 'w') as file:\n",
        "    file.write(modified_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
